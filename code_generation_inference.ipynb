{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c91e2bd3-4383-401d-b706-59f6f4c3f546",
   "metadata": {},
   "source": [
    "# Converting Code with Open-Source models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec1361-6796-456a-8ff0-f5b57c13929a",
   "metadata": {},
   "source": [
    "### Big Code Models Leaderboard\n",
    "https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard    \n",
    "- Look at the base model (filter)\n",
    "- And then \"All Models\"\n",
    "- Python and C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb44e46-9a45-480e-9ec7-50f85b47c7c4",
   "metadata": {},
   "source": [
    "| all | base | instruction-tuned | EXT external-evaluation | \n",
    "| --- | --- | --- | --- |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febfbebb-44e0-485c-a243-b27836ae9c1b",
   "metadata": {},
   "source": [
    "| T | Model | Win Rate | humaneval-python | java | javascript | cpp | \n",
    "| --- | --- | --- | --- | --- | --- | --- | \n",
    "| EXT | Qwen2.5-Coder-32B-Instruct | 59.17 | 83.2 | 73.69 | 76.05 | 81.95 |    \n",
    "      \n",
    "EXT - benchmark was done externally (Select the ones which does not have this)         \n",
    "However, Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98958a7f-67b1-418a-8c20-b9bc98c8eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"write a quick sort algorithm.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5995d98-772a-428a-82b4-7df9e7b2f804",
   "metadata": {},
   "source": [
    "Ask huggingFace to run this model for you and to give you an endpoint which you can use to call the model remotely from your code.      \n",
    "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct     \n",
    "\"Deploy\" Dropdown Selct \"Inference EndPoints(dedicated\" (I assume this is now changed to \"HF inference EndPoint\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be96c0-854c-49eb-b774-8881eea1d937",
   "metadata": {},
   "source": [
    "Unable to set it up as the lower cost ones are not available. $8 an hour is expensive. So did not try    \n",
    "     \n",
    "Tried the \"Inference provider\", the new option and see if it works. However, it looks like another solution is provided in the community section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5050978-fff6-4c0b-a2be-accc1943ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "\tprovider=\"together\",\n",
    "\tapi_key=\"hf_xxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-Coder-32B-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f05c21-6f42-4627-94d6-83a1d38d8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SambaNova   \n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "\tprovider=\"sambanova\",\n",
    "\tapi_key=\"hf_xxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-Coder-32B-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31c260-4116-48f4-94cc-aebee16dbb62",
   "metadata": {},
   "source": [
    "# How to evaluate the performance of a Gen AI solution?      \n",
    "Two different kind of performance metrics     \n",
    "- Model Centric or Technical\n",
    "- Business-centric or Outcome Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9fc8e-a154-4fb7-9839-19098b35c17c",
   "metadata": {},
   "source": [
    "## Model centric or Technical Metrics       \n",
    "kind of metrics that data scientists live and breathe by because these are metrics which we can optimize our models with and they tend to measure in a very immediate way the performance of the model              \n",
    "- Loss (e.g. cross entropy loss)\n",
    "- Perplexity\n",
    "- Accuracy\n",
    "- Precision, Recall, F1\n",
    "- AUC-ROC\n",
    "      \n",
    "Easiest to optimize with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b4ce0-4394-477d-a137-8cb4daf2f027",
   "metadata": {},
   "source": [
    "### Loss    \n",
    "How poorly an LLM has performed in its task which is used during optimization of the training and try to reduce it    \n",
    "Cross-Entropy Loss is most frequently used.   \n",
    "Input set/sequence of tokens and try to predict the next token and you have the real token and calculate the loss.      \n",
    "What the model actually does is it just doesn't predict the next token but gives you a probability distribution of the possible next tokens that could come in the list.    \n",
    "We may pick the highest probability and what the model predict and that probability is what we will use as the basis for the cross-entropy loss     \n",
    "(We take the negative log, we take minus the log of the probability. If you take the probability of one, which would be a perfect answer, it would mean that we said there was a 100% likelihood that the next token was exactly the thing that turned out to be the next token. The negative log of one is zero. zero loss. perfect answer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11371080-832a-4eef-bc26-c7d1ce8d5597",
   "metadata": {},
   "source": [
    "### Perplexity   \n",
    "\n",
    "The power of cross-entropy loss. The perplexity of one would mean that the model is completly confident and correct in its results.     \n",
    "100% accurate and 100% certainty gives perplexity of one.  A perplexity of two mean its right half the time and perplexity of four means 25%       \n",
    "A higher perplexity gives you a sense of a how many tokens would need to be , if all things were equal, in order to predict the next token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9303646-5dcd-4dd8-8e6c-aca698eb982b",
   "metadata": {},
   "source": [
    "## Business-centric or Outcome Metrics    \n",
    "These ones resonate most with the business audience and ultimately the provlem that they are asking you to solve     \n",
    "\n",
    "- KPIs tied to business objectives\n",
    "- ROI\n",
    "- Improvemens in time, cost or resources\n",
    "- Customer satisfaction\n",
    "- Benchmark comparisons\n",
    "     \n",
    "Most tangible impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ebc8e-142f-42a7-ad57-fc14b798c014",
   "metadata": {},
   "source": [
    "## Challenges    \n",
    "     \n",
    "- A code tool that automatically adds docstring/eomments\n",
    "- A code gen tool that writes unit test cases\n",
    "- A code generator that writes trading code to buy and sell equities in a simulated environment, based on a given API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c176a1e-920d-4e30-8fb5-52940c7a739a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a80b9-249a-42ba-86c8-94c3d2cbd9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61db42ef-f573-4559-a923-43ee03d525e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2288f0c1-8125-41df-bcca-1e8f293fd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import subprocess, re   \n",
    "\n",
    "from huggingface_hub import login, InferenceClient\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a449e878-7bea-45a8-8b6c-131256588573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "# os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "\n",
    "hf_token = os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755759be-0a82-413d-b18f-0f84346ceb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()   \n",
    "\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea5cf436-77a2-4575-97f0-65f1a72ba83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20240620\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc2321c-7e11-4623-997b-04e1acd33193",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that reimplements Python code in high performance for windows with Intel CPU. \"\n",
    "system_message += \"Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. \"\n",
    "system_message += \"The C++ response needs to produce an identical output in the fastest possible time. Keep implementations of random number generators identical so that results match exactly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "223293f1-7fd1-4601-9d3b-42c954412e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(python):\n",
    "    user_prompt = \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
    "    user_prompt += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
    "    user_prompt += \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
    "    user_prompt += python\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccddd6ff-97be-4dad-8b7a-8d0ec96ae9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e22f49f5-01ce-433f-acae-53e35e4b275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a file called optimized.cpp\n",
    "\n",
    "def write_output(cpp):\n",
    "    code = cpp.replace(\"```cpp\",\"\").replace(\"```\",\"\")\n",
    "    with open(\"optimized.cpp\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b3dcd16-e777-4891-9b3d-fcb796824ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383a24c7-9bcf-4718-bec6-a00586d30192",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_hard = \"\"\"# Be careful to support large number sizes\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2986f5d7-81fd-4898-b133-dd891b194216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(python):    \n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(python), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('```cpp\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4db12607-e03b-4f52-91de-049bcb7c1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(python):\n",
    "    result = claude.messages.stream(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=2000,\n",
    "        system=system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt_for(python)}],\n",
    "    )\n",
    "    reply = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            reply += text\n",
    "            yield reply.replace('```cpp\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261507c1-b55d-4963-a986-64d063224172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952d396-4a47-4c11-888e-af2fd043831e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2228a50-fe9b-4282-92da-b4db6e559026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using inference providers - QWEN\n",
    "def stream_code_qwen(python):\n",
    "    messages = messages_for(python)\n",
    "    client = InferenceClient(\n",
    "    \tprovider=\"sambanova\",\n",
    "    \tapi_key=hf_token\n",
    "    )\n",
    "    stream = client.chat.completions.create(\n",
    "    \tmodel=\"Qwen/Qwen2.5-Coder-32B-Instruct\", \n",
    "    \tmessages=messages, \n",
    "    \tmax_tokens=500,\n",
    "    \tstream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            result += chunk.choices[0].delta.content\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f90dcf-a5df-49a0-8cf0-27550c377f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910e824-3493-4756-8ee4-99e01efda33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b28648-0b6c-449c-ba41-a6a0367effb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ed48a-c191-4a81-889f-8c8651da856c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b353630-9498-46a3-9d4b-ff9e5ed1c410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4fa338-aa11-4ec8-b313-922de906898d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2849610f-a97e-48f2-bc95-2838b8ba99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using inference providers\n",
    "def stream_code_inference(python, model):\n",
    "    messages = messages_for(python)\n",
    "    client = InferenceClient(\n",
    "    \tprovider=\"sambanova\",\n",
    "    \tapi_key=hf_token\n",
    "    )\n",
    "    stream = client.chat.completions.create(\n",
    "    \tmodel= model, # \"Qwen/Qwen2.5-Coder-32B-Instruct\", \n",
    "    \tmessages=messages, \n",
    "    \tmax_tokens=500,\n",
    "    \tstream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            result += chunk.choices[0].delta.content\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe45ace8-0b02-4c80-bb23-e165bed0c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(python, model):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(python)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(python)\n",
    "    elif model==\"CodeQwen\":\n",
    "        result = stream_code_qwen(python)\n",
    "        result = stream_code_inference(python, \"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36e796ef-12bb-4400-9f28-0019f35bf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sample_program(sample_program):\n",
    "    if sample_program==\"pi\":\n",
    "        return pi\n",
    "    elif sample_program==\"python_hard\":\n",
    "        return python_hard\n",
    "    else:\n",
    "        return \"Type your Python program here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9565f84-99b8-4af6-8349-c4d3f771defd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e71af-0c0a-40b6-9bbf-a75cc6ce63e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5c56d-e46f-4cc6-b7aa-ca99cf1cad99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a452bbd-875c-48aa-800e-6cdfb115dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_python(code):\n",
    "    try:\n",
    "        output = io.StringIO()\n",
    "        sys.stdout = output\n",
    "        exec(code)\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__\n",
    "    return output.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fb7063e-02ec-4817-9234-41b2034e4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_cpp(code):\n",
    "    write_output(code)\n",
    "    try:\n",
    "        compile_result = subprocess.run(compiler_cmd[2], check=True, text=True, capture_output=True)\n",
    "        run_cmd = [\"./optimized\"]\n",
    "        run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)\n",
    "        return run_result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41792c06-814f-44b4-8219-9cf0c01a0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "VISUAL_STUDIO_2022_TOOLS = \"C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\Common7\\Tools\\\\VsDevCmd.bat\"\n",
    "VISUAL_STUDIO_2019_TOOLS = \"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\Common7\\\\Tools\\\\VsDevCmd.bat\"\n",
    "GCC_COMPILER = \"C:\\\\msys64\\\\ucrt64\\\\bin\\\\g++.exe\"\n",
    "\n",
    "simple_cpp = \"\"\"\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "    std::cout << \"Hello\";\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def run_cmd(command_to_run):\n",
    "    try:\n",
    "        run_result = subprocess.run(command_to_run, check=True, text=True, capture_output=True)\n",
    "        return run_result.stdout if run_result.stdout else \"SUCCESS\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def c_compiler_cmd(filename_base):\n",
    "    my_platform = platform.system()\n",
    "    my_compiler = []\n",
    "\n",
    "    try:\n",
    "        with open(\"simple.cpp\", \"w\") as f:\n",
    "            f.write(simple_cpp)\n",
    "            \n",
    "        if my_platform == \"Windows\":\n",
    "            \n",
    "            if os.path.isfile(VISUAL_STUDIO_2022_TOOLS):\n",
    "                if os.path.isfile(\"./simple.exe\"):\n",
    "                    os.remove(\"./simple.exe\")\n",
    "                compile_cmd = [\"cmd\", \"/c\", VISUAL_STUDIO_2022_TOOLS, \"&\", \"cl\", \"simple.cpp\"]\n",
    "                if run_cmd(compile_cmd):\n",
    "                    if run_cmd([\"./simple.exe\"]) == \"Hello\":\n",
    "                        my_compiler = [\"Windows\", \"Visual Studio 2022\", [\"cmd\", \"/c\", VISUAL_STUDIO_2022_TOOLS, \"&\", \"cl\", f\"{filename_base}.cpp\"]]\n",
    "\n",
    "            if os.path.isfile(GCC_COMPILER):\n",
    "                if os.path.isfile(\"./simple.exe\"): \n",
    "                    os.remove(\"./simple.exe\")\n",
    "                compile_cmd = [\"g++\", \"simple.cpp\", \"-o\", \"simple\"]\n",
    "                if run_cmd(compile_cmd):\n",
    "                    if run_cmd([\"./simple\"]) == \"Hello\":\n",
    "                        my_compiler = [\"Windows\", \"GCC (g++)\", [\"g++\", f\"{filename_base}.cpp\", \"-o\", f\"{filename_base}\" ]] \n",
    "        \n",
    "            if not my_compiler:\n",
    "                if os.path.isfile(VISUAL_STUDIO_2019_TOOLS):\n",
    "                    if os.path.isfile(\"./simple.exe\"):\n",
    "                        os.remove(\"./simple.exe\")\n",
    "                    compile_cmd = [\"cmd\", \"/c\", VISUAL_STUDIO_2019_TOOLS, \"&\", \"cl\", \"simple.cpp\"]\n",
    "                    if run_cmd(compile_cmd):\n",
    "                        if run_cmd([\"./simple.exe\"]) == \"Hello\":\n",
    "                            my_compiler = [\"Windows\", \"Visual Studio 2019\", [\"cmd\", \"/c\", VISUAL_STUDIO_2019_TOOLS, \"&\", \"cl\", f\"{filename_base}.cpp\"]]\n",
    "    \n",
    "            if not my_compiler:\n",
    "                my_compiler=[my_platform, \"Unavailable\", []]\n",
    "                \n",
    "        elif my_platform == \"Linux\":\n",
    "            if os.path.isfile(\"./simple\"):\n",
    "                os.remove(\"./simple\")\n",
    "            compile_cmd = [\"g++\", \"simple.cpp\", \"-o\", \"simple\"]\n",
    "            if run_cmd(compile_cmd):\n",
    "                if run_cmd([\"./simple\"]) == \"Hello\":\n",
    "                    my_compiler = [\"Linux\", \"GCC (g++)\", [\"g++\", f\"{filename_base}.cpp\", \"-o\", f\"{filename_base}\" ]]\n",
    "    \n",
    "            if not my_compiler:\n",
    "                if os.path.isfile(\"./simple\"):\n",
    "                    os.remove(\"./simple\")\n",
    "                compile_cmd = [\"clang++\", \"simple.cpp\", \"-o\", \"simple\"]\n",
    "                if run_cmd(compile_cmd):\n",
    "                    if run_cmd([\"./simple\"]) == \"Hello\":\n",
    "                        my_compiler = [\"Linux\", \"Clang++\", [\"clang++\", f\"{filename_base}.cpp\", \"-o\", f\"{filename_base}\"]]\n",
    "        \n",
    "            if not my_compiler:\n",
    "                my_compiler=[my_platform, \"Unavailable\", []]\n",
    "    \n",
    "        elif my_platform == \"Darwin\":\n",
    "            if os.path.isfile(\"./simple\"):\n",
    "                os.remove(\"./simple\")\n",
    "            compile_cmd = [\"clang++\", \"-Ofast\", \"-std=c++17\", \"-march=armv8.5-a\", \"-mtune=apple-m1\", \"-mcpu=apple-m1\", \"-o\", \"simple\", \"simple.cpp\"]\n",
    "            if run_cmd(compile_cmd):\n",
    "                if run_cmd([\"./simple\"]) == \"Hello\":\n",
    "                    my_compiler = [\"Macintosh\", \"Clang++\", [\"clang++\", \"-Ofast\", \"-std=c++17\", \"-march=armv8.5-a\", \"-mtune=apple-m1\", \"-mcpu=apple-m1\", \"-o\", f\"{filename_base}\", f\"{filename_base}.cpp\"]]\n",
    "    \n",
    "            if not my_compiler:\n",
    "                my_compiler=[my_platform, \"Unavailable\", []]\n",
    "    except:\n",
    "        my_compiler=[my_platform, \"Unavailable\", []]\n",
    "        \n",
    "    if my_compiler:\n",
    "        return my_compiler\n",
    "    else:\n",
    "        return [\"Unknown\", \"Unavailable\", []]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5ad5166-ccb5-4946-85a9-f152c5a8b89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Windows', 'GCC (g++)', ['g++', 'simple.cpp.cpp', '-o', 'simple.cpp']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_compiler_cmd(\"simple.cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87950893-eb44-48ce-ac89-02f3a2bd468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".python {background-color: #306998;}\n",
    ".cpp {background-color: #050;}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19af5373-58b2-4e8c-84e6-ca7a67754f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiler_cmd = c_compiler_cmd(\"optimized\")\n",
    "\n",
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\"## Convert code from Python to C++\")\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", value=python_hard, lines=10)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            sample_program = gr.Radio([\"pi\", \"python_hard\"], label=\"Sample program\", value=\"python_hard\")\n",
    "            model = gr.Dropdown([\"GPT\", \"Claude\", \"CodeQwen\", ], label=\"Select model\", value=\"GPT\")\n",
    "        with gr.Column():\n",
    "            architecture = gr.Radio([compiler_cmd[0]], label=\"Architecture\", interactive=False, value=compiler_cmd[0])\n",
    "            compiler = gr.Radio([compiler_cmd[1]], label=\"Compiler\", interactive=False, value=compiler_cmd[1])\n",
    "    with gr.Row():\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "    with gr.Row():\n",
    "        python_run = gr.Button(\"Run Python\")\n",
    "        if not compiler_cmd[1] == \"Unavailable\":\n",
    "            cpp_run = gr.Button(\"Run C++\")\n",
    "        else:\n",
    "            cpp_run = gr.Button(\"No compiler to run C++\", interactive=False)\n",
    "    with gr.Row():\n",
    "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])\n",
    "        cpp_out = gr.TextArea(label=\"C++ result:\", elem_classes=[\"cpp\"])\n",
    "\n",
    "    sample_program.change(select_sample_program, inputs=[sample_program], outputs=[python])\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "    python_run.click(execute_python, inputs=[python], outputs=[python_out])\n",
    "    cpp_run.click(execute_cpp, inputs=[cpp], outputs=[cpp_out])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc052c37-400b-44a3-8ea0-ea0621129c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
