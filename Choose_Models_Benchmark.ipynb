{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93af39ff-58ed-4797-befb-245ae0764db4",
   "metadata": {},
   "source": [
    "# LLM Showdown: Evaluating Models for Code Generation   \n",
    "\n",
    "- Build a multi-modal AI Assistant with Tools\n",
    "- Build Solutions with open-source LLMs with HuggingFace transformers\n",
    "\n",
    "         \n",
    "- How to select the right LLM for the task\n",
    "- Comapare LLMs based on their basic attributes and benchmarkss\n",
    "- Use the open LLM Keaderboard to evaluate LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7d0b7-b7c6-4f0b-abd4-f9470773450c",
   "metadata": {},
   "source": [
    "![](images/plan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736e4d9-6fed-41fc-85b0-262c9991d8d9",
   "metadata": {},
   "source": [
    "#### Start with the basics \n",
    "Parameters, Context Length, Pricing      \n",
    "      \n",
    "\n",
    "#### Look at the results \n",
    "Benchmarks, Leaderboards, Arenas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aca4cc-d409-4a29-99f5-eede93e0cbeb",
   "metadata": {},
   "source": [
    "### The Basics (1)\n",
    "- Open-Source or closed\n",
    "- Release date and knowledge cut-off\n",
    "- Parameters\n",
    "- Training Tokens\n",
    "- Context Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a428901-d72c-4de5-9e61-b25e3c228514",
   "metadata": {},
   "source": [
    "### The Basics (2) \n",
    "- Inference cost\n",
    "     - API charge, Subscription or Runtime compute\n",
    "- Training cost\n",
    "- Build cost\n",
    "- Time to Market\n",
    "- Rate Limits\n",
    "- Speed\n",
    "- Latency\n",
    "- License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85878a8e-15c9-491b-a497-0209db2ee650",
   "metadata": {},
   "source": [
    "### The Chinchilla Scaling Law \n",
    "   \n",
    "Number of parameters ~ proportional to the number of training tokens      \n",
    "     \n",
    "If you're getting diminishing returns from training with more training data, then this law gives you a rule of thumb for scaling your model       \n",
    "     \n",
    "And vice versa: If you upgrade to a model with double the number of weights, this law indicates your training data requirement.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c9496-6138-483b-9c26-de04701557ca",
   "metadata": {},
   "source": [
    "### 7 Common benchmarks    \n",
    "      \n",
    "| Benchmark | What's being evaluated | Description | \n",
    "| --- | --- | --- | \n",
    "| ARC | Reasoning | A benchmark for evaluating scientific reasonin; multiple-choice questions |\n",
    "| DROP | Language Comp | Distill details from text then add, count or sort | \n",
    "| HellaSwag | Common Sense | \"Harder Endings, Long Contexts and Low Shot Activities\" | \n",
    "| MMLU | Understanding | Factual recall, reasoning and problem solving across 57 subjects | \n",
    "} TruthfulQA | Accuracy | Robustness in providing truthful replies in adversarial conditions | \n",
    "| Winogrande | Context | Test the LLM understands context and resolves ambiguity | \n",
    "| GSM8D | Math | Math and word problems taught in elementary and middle schools|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae4c3c-ed88-4238-8513-bc4bf3d9eb5c",
   "metadata": {},
   "source": [
    "### 3 Specific benchmarks  \n",
    "     \n",
    "| Benchmark | What's being evaluated | Description | \n",
    "| --- | --- | --- | \n",
    "| ELO | Chat | Results from head-to=head face-offs with other LLMs, as with ELO in Chess | \n",
    "| HumanEval | Python Coding | 164 Problems writing code based on docstrings | \n",
    "| MultiPL-E | Broader Coding | Translation of HumanEval to 18 Programming Languages | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b37d01-2c6c-40aa-8b73-f90da5b17dbc",
   "metadata": {},
   "source": [
    "### Limitations of Benchmarks \n",
    "     \n",
    "- Not consistantly applied\n",
    "    - (Press release from the company, it is upto the company how they measure the benchmark, hardware they use etc. (be skeptical)\n",
    "- Too narrow in scope\n",
    "    - (Multiple choice questions  \n",
    "- Hard to measure nuanced reasoning\n",
    "    - (Multipe choice questions or very specific kind of question and answers.\n",
    "- Training data leakage\n",
    "    - (difficult to ascertain that the answers can be found within the data that is being used to train models. \n",
    "- Overfitting\n",
    "    - (trained so well, the model may have difficulty responding to a question asked bit differently or other type of questions\n",
    "\n",
    "And a new conern, not yet proven    \n",
    "- Frontier LLMs may be aware that they are being evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd444df6-fb18-4312-a140-25337b6b50eb",
   "metadata": {},
   "source": [
    "### 6 Next Level Benchmarks  \n",
    "     \n",
    "| Benchmark | What's being evaluated | Description | \n",
    "| --- | --- | --- | \n",
    "| GPQA | Graduate Tests | 448 expert questions; non PHD humans score 34% even with web access |      \n",
    "| BBHard | Future Capabilities | 204 tasks believed beyond capabilities of LLMs (No longer) | \n",
    "| Math Lv 5 | Math | High-school level math competition problems | \n",
    "| IFEval | Difficult Instructions | Like, \"write more than 400 words\" and \"mention AI at least 3 times\" | \n",
    "| MuSR | Multistep Soft Reasoning | Logical deduction, such as analyzing 1000 word murder mystery and answering \"Who has means, motive and opportunity?\" | \n",
    "| MMLU-PRO | Harder MMLU | A more advanced and cleaned up version of MMLU including choice of 10 answers instead of 4 | \n",
    "\n",
    "\n",
    "GPQA - Google Proof Q and A  (Non PHD people will score 34% (PHD 64%)  even if you have access to web \"GOOGLE\") - Claude 3.5 Sonnet was the leading model 59%          \n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fb490-ced3-4818-844d-7d04b0bc81ec",
   "metadata": {},
   "source": [
    "### Open LLM Leaderboard \n",
    "    \n",
    "     \n",
    "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/     \n",
    "(Previous - https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard)\n",
    "       \n",
    "#### Model Types  (Advaned Filter Section)\n",
    "- fine-tuned on domain-specific datasets 1414\n",
    "- chat models (RLHF, DP, iFT, ...)  690\n",
    "- base merges and moerges   1234 \n",
    "- pretrained   272\n",
    "- continuously pretrained 0\n",
    "- Multimodal 7\n",
    "     \n",
    "#### Parameters    \n",
    "All or select the parameters 1-7B, 70B, 140B        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394325be-8697-4ebf-9727-ebb0e7dfe3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
