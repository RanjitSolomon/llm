{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93af39ff-58ed-4797-befb-245ae0764db4",
   "metadata": {},
   "source": [
    "# LLM Showdown: Evaluating Models for Code Generation   \n",
    "\n",
    "- Build a multi-modal AI Assistant with Tools\n",
    "- Build Solutions with open-source LLMs with HuggingFace transformers\n",
    "\n",
    "         \n",
    "- How to select the right LLM for the task\n",
    "- Comapare LLMs based on their basic attributes and benchmarkss\n",
    "- Use the open LLM Keaderboard to evaluate LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7d0b7-b7c6-4f0b-abd4-f9470773450c",
   "metadata": {},
   "source": [
    "![](images/plan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736e4d9-6fed-41fc-85b0-262c9991d8d9",
   "metadata": {},
   "source": [
    "#### Start with the basics \n",
    "Parameters, Context Length, Pricing      \n",
    "      \n",
    "\n",
    "#### Look at the results \n",
    "Benchmarks, Leaderboards, Arenas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aca4cc-d409-4a29-99f5-eede93e0cbeb",
   "metadata": {},
   "source": [
    "### The Basics (1)\n",
    "- Open-Source or closed\n",
    "- Release date and knowledge cut-off\n",
    "- Parameters\n",
    "- Training Tokens\n",
    "- Context Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a428901-d72c-4de5-9e61-b25e3c228514",
   "metadata": {},
   "source": [
    "### The Basics (2) \n",
    "- Inference cost\n",
    "     - API charge, Subscription or Runtime compute\n",
    "- Training cost\n",
    "- Build cost\n",
    "- Time to Market\n",
    "- Rate Limits\n",
    "- Speed\n",
    "- Latency\n",
    "- License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85878a8e-15c9-491b-a497-0209db2ee650",
   "metadata": {},
   "source": [
    "### The Chinchilla Scaling Law \n",
    "   \n",
    "Number of parameters ~ proportional to the number of training tokens      \n",
    "     \n",
    "If you're getting diminishing returns from training with more training data, then this law gives you a rule of thumb for scaling your model       \n",
    "     \n",
    "And vice versa: If you upgrade to a model with double the number of weights, this law indicates your training data requirement.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c9496-6138-483b-9c26-de04701557ca",
   "metadata": {},
   "source": [
    "### 7 Common benchmarks    \n",
    "      \n",
    "| Benchmark | What's being evaluated | Description | \n",
    "| --- | --- | --- | \n",
    "| ARC | Reasoning | A benchmark for evaluating scientific reasonin; multiple-choice questions |\n",
    "| DROP | Language Comp | Distill details from text then add, count or sort | \n",
    "| HellaSwag | Common Sense | \"Harder Endings, Long Contexts and Low Shot Activities\" | \n",
    "| MMLU | Understanding | Factual recall, reasoning and problem solving across 57 subjects | \n",
    "} TruthfulQA | Accuracy | Robustness in providing truthful replies in adversarial conditions | \n",
    "| Winogrande | Context | Test the LLM understands context and resolves ambiguity | \n",
    "| GSM8D | Math | Math and word problems taught in elementary and middle schools|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae4c3c-ed88-4238-8513-bc4bf3d9eb5c",
   "metadata": {},
   "source": [
    "### 3 Specific benchmarks  \n",
    "     \n",
    "| Benchmark | What's being evaluated | Description | \n",
    "| --- | --- | --- | \n",
    "| ELO | Chat | Results from head-to=head face-offs with other LLMs, as with ELO in Chess | \n",
    "| HumanEval | Python Coding | 164 Problems writing code based on docstrings | \n",
    "| MultiPL-E | Broader Coding | Translation of HumanEval to 18 Programming Languages | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b37d01-2c6c-40aa-8b73-f90da5b17dbc",
   "metadata": {},
   "source": [
    "### Limitations of Benchmarks \n",
    "     \n",
    "- Not consistantly applied\n",
    "    - (Press release from the company, it is upto the company how they measure the benchmark, hardware they use etc. (be skeptical)\n",
    "- Too narrow in scope\n",
    "    - (Multiple choice questions  \n",
    "- Hard to measure nuanced reasoning\n",
    "    - (Multipe choice questions or very specific kind of question and answers.\n",
    "- Training data leakage\n",
    "    - (difficult to ascertain that the answers can be found within the data that is being used to train models. \n",
    "- Overfitting\n",
    "    - (trained so well, the model may have difficulty responding to a question asked bit differently or other type of questions\n",
    "\n",
    "And a new conern, not yet proven    \n",
    "- Frontier LLMs may be aware that they are being evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd444df6-fb18-4312-a140-25337b6b50eb",
   "metadata": {},
   "source": [
    "### 6 Next Level Benchmarks  \n",
    "     \n",
    "| Benchmark | What's being evaluated | Description | \n",
    "| --- | --- | --- | \n",
    "| GPQA | Graduate Tests | 448 expert questions; non PHD humans score 34% even with web access |      \n",
    "| BBHard | Future Capabilities | 204 tasks believed beyond capabilities of LLMs (No longer) | \n",
    "| Math Lv 5 | Math | High-school level math competition problems | \n",
    "| IFEval | Difficult Instructions | Like, \"write more than 400 words\" and \"mention AI at least 3 times\" | \n",
    "| MuSR | Multistep Soft Reasoning | Logical deduction, such as analyzing 1000 word murder mystery and answering \"Who has means, motive and opportunity?\" | \n",
    "| MMLU-PRO | Harder MMLU | A more advanced and cleaned up version of MMLU including choice of 10 answers instead of 4 | \n",
    "\n",
    "\n",
    "GPQA - Google Proof Q and A  (Non PHD people will score 34% (PHD 64%)  even if you have access to web \"GOOGLE\") - Claude 3.5 Sonnet was the leading model 59%          \n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fb490-ced3-4818-844d-7d04b0bc81ec",
   "metadata": {},
   "source": [
    "### Open LLM Leaderboard \n",
    "    \n",
    "     \n",
    "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/     \n",
    "(Previous - https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard)\n",
    "       \n",
    "#### Model Types  (Advaned Filter Section)\n",
    "- fine-tuned on domain-specific datasets 1414\n",
    "- chat models (RLHF, DP, iFT, ...)  690\n",
    "- base merges and moerges   1234 \n",
    "- pretrained   272\n",
    "- continuously pretrained 0\n",
    "- Multimodal 7\n",
    "     \n",
    "#### Parameters    \n",
    "All or select the parameters 1-7B, 70B, 140B        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023fad3-d18b-43ee-a04e-3c456c141ccc",
   "metadata": {},
   "source": [
    "### Six Leaderboards    \n",
    "    \n",
    "- HuggingFace Open LLM\n",
    "    - New and old version\n",
    "- HuggingFace BigCode\n",
    "- HuggingFace LLM Perf\n",
    "- HuggingFace Others\n",
    "    - i.e, Medical Language specific\n",
    "- Vellum\n",
    "    - Includes API cost and context window\n",
    "- SEAL\n",
    "    - expert skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9615916-d32c-4f1e-8a3a-05a2379ceecc",
   "metadata": {},
   "source": [
    "### The Arena \n",
    "    \n",
    "- Compare Frontier and Open-source models directly\n",
    "- Blind human evals based on head-to-head comparison\n",
    "- LLMs are measured with an ELO rating "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5af461-659b-410a-b227-5e231cc193a4",
   "metadata": {},
   "source": [
    "### Commercial Use Cases \n",
    "     \n",
    "- Law  (Harvey)\n",
    "- Talent (nebula.io)\n",
    "- Porting code (Bloop)\n",
    "- Healthcare (Salesforce Health)\n",
    "- Education (Khanmigo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b2467-daac-4a15-bfff-eee7dde8ede3",
   "metadata": {},
   "source": [
    "### Big Code Models Leaderboard \n",
    "https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard      \n",
    "     \n",
    "Filter model types - Base        \n",
    "Top model as at Feb 7, 2025        \n",
    "- Qwen2.5-Coder-32B (Win Rate 56.67, Humaneval-python 57.1, java 65.49, javascript 65.87 cpp 64.35)\n",
    "     \n",
    "About tab - How the ratings are calculated     \n",
    "     \n",
    "- Win Rate represents how often a model outperforms other models in each language, averaged across all languages.\n",
    "- The scores of instruction-tuned models might be significantly higher on humaneval-python than other languages. We use the instruction format of HumanEval. For other languages, we use base MultiPL-E prompts.\n",
    "- For more details check the üìù About section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910da23e-e4ce-4a61-a48e-d5f432bd91e1",
   "metadata": {},
   "source": [
    "### LLM-Perf Leaderboard    \n",
    "https://huggingface.co/spaces/optimum/llm-perf-leaderboard      \n",
    "\n",
    "Hardware <b>\"A10-24GB-150W\"</b>     \n",
    "      \n",
    "Select \"Find Your Best Model\" Tab      \n",
    "Latency vs Score vs Memory diagram    \n",
    "     \n",
    "X axis - \"Time To Generate 64 Tokens\"  - Speed - the ones on the left           \n",
    "Y axis - \"Open LLM Score(%)\" - Accuracy  The higher the better      \n",
    "Cost - Size of the Blob - small is better          \n",
    "Color - Family of the model    \n",
    "      \n",
    "Qwen/Qwen1.5-1.8B       \n",
    "X axis - Left, y axis - somewhere in the lower middle, size-small           \n",
    "      \n",
    "Phi3-mini-4k-instruct  (X-left, y=high, size-small)     \n",
    "      \n",
    "Select the Hardware <b>\"T4-16GB-70W\"</b> to see the available options         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b97279-b8f4-481d-87fa-ff1671f8a61f",
   "metadata": {},
   "source": [
    "### Spaces     \n",
    "search for \"Leaderboard\"       \n",
    "https://huggingface.co/spaces?search=leaderboard         \n",
    "     \n",
    "Medical Leaderboard   \n",
    "https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard      \n",
    "MMLU Clinical Knowledge     \n",
    "MMLU College Biology     \n",
    "MMLU Medical Genetics    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d8ad3-8695-4300-8ed0-d105fcd613b8",
   "metadata": {},
   "source": [
    "### Vellum Leaderboard \n",
    "https://www.vellum.ai/llm-leaderboard     \n",
    "Open and Close Source Leaderboard      \n",
    "\n",
    "#### Best in Multitask Reasoning (MMLU)         \n",
    "OpenAI o1       \n",
    "DeepSeek-R1     \n",
    "GPT-4o       \n",
    "Llama-405B       \n",
    "       \n",
    "#### Best in Coding (Human Eval)    \n",
    "OpenAI o1     \n",
    "3.5 Sonnet      \n",
    "GPT-4o     \n",
    "Llama 405B     \n",
    "     \n",
    "#### Best in Math (MATH)     \n",
    "o3-mini    \n",
    "DeepSeek-R1     \n",
    "o1     \n",
    "DeepSeek-v3        \n",
    "    \n",
    "#### Fastest and Most affordable    \n",
    "Llama 70B      \n",
    "      \n",
    "#### Lowest Latency (TTFT) \n",
    "(Seconds to First tokens chunk received - lower is better)     \n",
    "Llama 8B      \n",
    "      \n",
    "#### Cheapest \n",
    "Nova Mico      \n",
    "     \n",
    "#### Compare Models       \n",
    "Select two models to compare\n",
    "       \n",
    "#### Model Comparison     \n",
    "List of models and ratings      \n",
    "     \n",
    "      \n",
    "| Model | Average | MMLU(General) | GPQA (Reasoning) | HumanEval(Coding) | Math | BFCL(ToolUse) | MGSM(Multilingual) | \n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |    \n",
    "| OpenAI o1 | 85.39% | 91.8% | 75.7% | 92.4% | 96.4% | 66.73% | 89.3% |      \n",
    "| Claude 3.5 Sonnet | 84.5% | 88.3% | 65% | 93.7% | 78.3% | 90.2% | 91.6% |     \n",
    "      \n",
    "#### Cost and Context Window Comparison      \n",
    "      \n",
    "| Models | Context Window | Input Cost/1M tokens | Output cost/1M tokens | \n",
    "| --- | --- | --- | --- |     \n",
    "| Gemini 1.5 Flash | 1,000,000 | 0.35 | 0.70 |      \n",
    "| "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff268f0-2135-4fb0-b9b1-a9fc576732f4",
   "metadata": {},
   "source": [
    "### SEAL   \n",
    "https://scale.com/leaderboard     \n",
    "      \n",
    "If you are working on a particular problem and you need to have a data set, crafted, curated for your problem      \n",
    "     \n",
    "This leaderboard has a bunch of very specific leaderboards for different tasks.       \n",
    "- MultiChallenge\n",
    "- Humanity's Last Exam\n",
    "- Visual Language Understanding\n",
    "- Chinese\n",
    "- Arabic\n",
    "- Koream\n",
    "- Japanese\n",
    "- Spanish\n",
    "- Agentic Tool Use (Enterprise)\n",
    "- Agentic Tool Use (Chat)\n",
    "- Coding\n",
    "- Math\n",
    "- Instruction Following\n",
    "- Adversarial Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f079368-2577-4483-8d35-6f862a1cc38f",
   "metadata": {},
   "source": [
    "### Chatbot Arean LLM Leaderboard   \n",
    "https://lmarena.ai/?leaderboard    \n",
    "     \n",
    "Chatbot Arena is an open platform for crowdsourced AI benchmarking, developed by researchers at UC Berkeley SkyLab and LMArena.    \n",
    "     \n",
    "| Rank(UB) | Rank(StyleCtrl) | Model | Arena Score | 95% CI | Votes | Organization | License |    \n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | \n",
    "| 1 | 2 |  Gemini-2.0-Flash-Thinking-Exp-01-21 | 1383 | +6/-7 | 10314 | Google | Proprietary |    \n",
    "     \n",
    "**Rank (UB):** model's ranking (upper-bound), defined by one + the number of models that are statistically better than the target model. Model A is statistically better than model B when A's lower-bound score is greater than B's upper-bound score (in 95% confidence interval). See Figure 1 below for visualization of the confidence intervals of model scores.     \n",
    "     \n",
    "Rank (StyleCtrl): model's ranking with style control, which accounts for factors like response length and markdown usage to decouple model performance from these potential confounding variables. See blog post for further details.       \n",
    "      \n",
    "Vote     \n",
    "https://lmarena.ai/    \n",
    "You will be presented with two unknown models A and B, Chat with it      \n",
    "\" Please tell me a light-hearted joke suitable for a room full of Data Scientists.\"     \n",
    "     \n",
    "Vote based on the response of A and B      \n",
    "      \n",
    "A is better, B is better, Tie, Both are bad        \n",
    "Both models will be shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a7c586-c741-40c6-a75c-e2bcafdedf5a",
   "metadata": {},
   "source": [
    "### Harvey Legal Platform \n",
    "https://www.harvey.ai/     \n",
    "     \n",
    "### Nebula \n",
    "https://nebula.io   \n",
    "Human resource     \n",
    "     \n",
    "### Bloop \n",
    "https://bloop.ai    \n",
    "port legacy code to Java    \n",
    "\n",
    "### Salesforce    \n",
    "https://www.salesforce.com/ca/artificial-intelligence/       \n",
    "     \n",
    "### Khanmigo   \n",
    "https://www.khanmigo.ai/     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ab4e80-2c0b-44b4-89f8-c8aeeacbb8ae",
   "metadata": {},
   "source": [
    "## Challenge    \n",
    "      \n",
    "####  Build a product that converts Python code to C++ for performance.     \n",
    "- Solution with a Frontier model\n",
    "- Solution with an Open-Source model\n",
    "- Start by selecting a LLMs most suited for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a19088-3fef-4cf7-ae66-04832bfff058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
