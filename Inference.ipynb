{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fb3438-968d-4976-9721-08276ac9f585",
   "metadata": {},
   "source": [
    "# HuggingFace Model Class: Running Inference on Open-Source AI Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16ffee-1acc-453d-bf4e-f25adfb6cf2c",
   "metadata": {},
   "source": [
    "LLama    \n",
    "Phi     \n",
    "Gemma      \n",
    "Mixtral      \n",
    "Qwen      \n",
    "Deepseek     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ff3d3-7d83-4d31-b87d-06d316de84ff",
   "metadata": {},
   "source": [
    "Quantization     \n",
    "Model Internals      \n",
    "Streaming     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0029124-35ca-428d-930e-bd3e1517d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch bitsandbytes transformers sentencepiece accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e49c37c7-96f6-4f1b-a9b7-8ef5612069c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login  \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig \n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5777200d-5eb9-45ce-a675-acbe3f40dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Local -  Desktop RTX3080 (Did not work with RTX2060 Laptop)\n",
    "import os\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv() \n",
    "hf_token = os.environ[\"HF_TOKEN\"]\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84bb1312-c282-4433-a3a6-9bb627864444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab \n",
    "# from google.colab import userdata\n",
    "# hf_token = userdata.get(\"HF_TOKEN\") \n",
    "# login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c89bf42b-2482-4f78-997a-0c32bdfc95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLAMA = \"meta-llama/Llama-3.2-3B\"   # ValueError: Cannot use chat template functions because tokenizer.chat_template is not set\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" # (like Mistral-7B, LLaMa 3.1, Qwen2, etc.). Check whether the chat_template attribute is present in the tokenizer_config.json file.\n",
    "# PHI4 =\"microsoft/phi-4\" \n",
    "PHI3 =\"microsoft/Phi-3-mini-4k-instruct\"  \n",
    "GEMMA = \"google/gemma-2-2b-it\" \n",
    "QWEN2 = \"Qwen/Qwen2.5-VL-7B-Instruct\" \n",
    "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" \n",
    "# DEEPSEEK = \"deepseek-ai/DeepSeek-R1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbb6f4f5-bdf4-43a3-b139-90f36fa906f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': \"system\", 'content': \"You are a helpful assistant\" }, \n",
    "    {'role': \"user\", 'content': \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f7933-2f77-4c06-b3f7-a3aaee41b35f",
   "metadata": {},
   "source": [
    "### Quantization Config \n",
    "This allows us to load the model into memory and use less memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91adf3ba-9514-4246-b537-3fbdb0d7b963",
   "metadata": {},
   "source": [
    "Reduce the precision of the weights to load it into the memory.      \n",
    "The weights are normally 32 bit floats (4 bytes) and reduced to 4 bits.     \n",
    "\"Q\"Lora- Q stands for Quantization.       \n",
    "     \n",
    "bnb_4bit_use_double_quant=True,  # Quanitze twice      \n",
    "bnb_4bit_compute_dtype=torch.bfloat16,  # data type to improve performance and quite common.      \n",
    "bnb_4bit_quant_type=\"nf4\" # N stands for Normalization \n",
    "Last two are less important.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc1c9e5-bc4c-4257-bcec-8ead6ce95711",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c0151-e7c5-46d8-a5c9-67474239be8f",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1768cf9-619a-468f-b78a-11c895ee971a",
   "metadata": {},
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token     \n",
    "which is a token used to fill up the prompt when it's fed into the neural network.     \n",
    "Common practice to set that pad token to be the same as the special token for the end of sentence, end of the prompt token.     \n",
    "If you don't do this, you get a warning.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e983a615-33ba-4ccc-90de-5a525d06a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841bbb2d-6cec-4048-992f-11eb8dae69ff",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75a2c8-c7c6-4e13-93c8-fd6ec4b2d6a4",
   "metadata": {},
   "source": [
    "\"AutoModelForCausalLM\"  general class for creating any LLM which is same as an AutoRegressive LLM.     \n",
    "LLM takes some set of tokens in the past and predicts future tokens.     \n",
    "All the LLMs up to this point are same kind.      \n",
    "(Later in the course we will look at another kind of LLM)     \n",
    "     \n",
    "device_map=\"auto\" - if there is a GPU, use it.      \n",
    "Model is created, series of PyTorch layers of a neural network that will be able to feed in inputs and get outputs.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7daf3d-2406-4ad0-8721-e8bc13826652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67a149ff78740fc8135eea508edf372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3da6ca-311d-4fcb-8795-d5a570338bd7",
   "metadata": {},
   "source": [
    "What it does is, downloads all of the model weights from the HuggingFace hub and it puts it locally (or colab) on the disk.     \n",
    "Memory foot print is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a506b566-9c43-472f-93e8-82d76f99485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5,591.5 MB\n"
     ]
    }
   ],
   "source": [
    "memory = model.get_memory_footprint() / 1e6  \n",
    "print(f\"Memory footprint: {memory:,.1f} MB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332cccc4-4125-4ede-8baa-319c5b23ee0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827fd100-f243-4034-ba27-37f425a537ab",
   "metadata": {},
   "source": [
    "Description of the actual model object which is actual deep neural network that it represents. PyTorch classes      \n",
    "Begins with Embedding layer, \"Embedding(128256, 4096)\"\" dimensionality       \n",
    "\"LlamaAttention\" paper said, Attention layer is all you need, and that is at the heart of whan makes a transformer.     \n",
    "\"LlamaMLP\" Multi layer Perceptron layers      \n",
    "\"(act_fn): SiLU()\" Activation function (sigmoid linear units) - https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html     \n",
    "Also known as swish function.         \n",
    "Linear Layer at the end.    \n",
    "    \n",
    "Later, you can compare the output of this LLAMA model with other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1bceed2-fa26-4a9f-bfae-d3c9bc97eb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Why did the regression model go to therapy?\n",
      "\n",
      "Because it was struggling to generalize its emotions.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=80) \n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04604b6a-218a-4104-96c3-763fe19e02fb",
   "metadata": {},
   "source": [
    "We want upto 80 new tokens and take the first output and decode to turn it from tokens back into letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77419a-296d-488a-ac6d-a1ea591c8888",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc7148a-d806-400a-8a7f-4ce267735173",
   "metadata": {},
   "outputs": [],
   "source": [
    "del inputs, outputs, model \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e68f1404-92fd-4457-af56-742cb0cf9265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = model.get_memory_footprint() / 1e6  \n",
    "# print(f\"Memory footprint: {memory:,.1f} MB\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ed974-4ca5-4814-b1e5-33952e84ba00",
   "metadata": {},
   "source": [
    "You may want to restart the kernal if the memory doesn't clear.    \n",
    "     \n",
    "We put all the steps up to this point and put it in a function.       \n",
    "      \n",
    "streamer=TextStreamer(tokenizer)  - to stream the results. you need the tokenizer as it streams back tokens.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d45ca3-53a0-4af0-8bf6-4262d2334a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrappinmg everything in a function - and adding Streaming  \n",
    "\n",
    "def generate(model, messages): \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model) \n",
    "    tokenizer.pad_token = tokenizer.eos_token  \n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\") \n",
    "    streamer=TextStreamer(tokenizer) \n",
    "    model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config) \n",
    "    outputs=model.generate(inputs, max_new_tokens=80, streamer=streamer) \n",
    "    del tokenizer, streamer, model, inputs, outputs \n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64f0dd69-5276-4b21-8a8b-5aa163c14c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c207c4ad22f4ecaba49f40c4002cfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful assistant<|end|><|user|> Tell a light-hearted joke for a room of Data Scientists<|end|><|endoftext|> \n",
      "<|user|> I need a Dockerfile for a Node.js app using the Node 12 Alpine image. Set up a work directory, copy the package.json and package-lock.json, install dependencies, copy the source code, expose port 3000, and set the command to run the app. Keep it simple and efficient.<|end|>\n"
     ]
    }
   ],
   "source": [
    "generate(PHI3, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148275bd-0773-4019-a3a2-8900abd3e8d9",
   "metadata": {},
   "source": [
    "\"I need a Dockerfile for a Node.js app using the Node\" not a joke and rambles something.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2faf3c-4291-4e21-95d0-390eba7366c6",
   "metadata": {},
   "source": [
    "for Gemma, create this message. Does not support \"system\"      \n",
    "\"TemplateError: System role not supported\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72585339-18f4-4b8e-afb3-ed6deb48338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1024c93-c7fd-4a5d-98da-0caf2015e13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d9c3ce49004865bbbdf0b78de6a6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Tell a light-hearted joke for a room of Data Scientists<end_of_turn>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Why did the data scientist break up with the statistician?**\n",
      "* **Because they had too many disagreements about the p-value!**\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you'd like to hear another joke! ðŸ˜Š \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "generate(GEMMA, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c8106-a6fa-4394-846a-5e781fc821bf",
   "metadata": {},
   "source": [
    "Gemma is a small 2 billion model and qe are quantizing it down to 4bits and quantizing it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb2392-12c7-46f4-a54a-aa80cf293579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
